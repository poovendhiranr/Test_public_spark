{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/poovendhiranr/Test_public_spark/blob/main/Test_spark.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EwTc4FoGwbPM",
        "outputId": "2a1c81ff-b703-4cc8-d5ee-62bc02159f2b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting pyspark\n",
            "  Downloading pyspark-3.3.1.tar.gz (281.4 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m281.4/281.4 MB\u001b[0m \u001b[31m5.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting py4j==0.10.9.5\n",
            "  Downloading py4j-0.10.9.5-py2.py3-none-any.whl (199 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m199.7/199.7 KB\u001b[0m \u001b[31m13.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hBuilding wheels for collected packages: pyspark\n",
            "  Building wheel for pyspark (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyspark: filename=pyspark-3.3.1-py2.py3-none-any.whl size=281845512 sha256=5e12ebbb504408bae097e6b541078e8e37782a6a772460f55d1cf3e2e10cf369\n",
            "  Stored in directory: /root/.cache/pip/wheels/43/dc/11/ec201cd671da62fa9c5cc77078235e40722170ceba231d7598\n",
            "Successfully built pyspark\n",
            "Installing collected packages: py4j, pyspark\n",
            "Successfully installed py4j-0.10.9.5 pyspark-3.3.1\n"
          ]
        }
      ],
      "source": [
        "!pip3 install pyspark\n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.types import StructType, StructField, IntegerType, StringType, LongType\n",
        "import math \n",
        "import pyspark.sql.functions as f\n",
        "from pyspark.sql import types \n",
        "from pyspark.sql import Window\n",
        "spark = SparkSession.builder.master(\"local[*]\") \\\n",
        "                    .appName('Test_spark') \\\n",
        "                    .getOrCreate()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "a=\"babad\"\n",
        "def find_longest_palindrome(s):\n",
        "    longest = ''\n",
        "    n = len(s)\n",
        "    for i in range(n):\n",
        "        for j in range(i+1,n+1):\n",
        "            word = s[i:j]\n",
        "            if word == word[::-1]:\n",
        "                if len(word)>len(longest):\n",
        "                    longest = word          \n",
        "    return longest\n",
        "\n",
        "find_longest_palindrome(a)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 217
        },
        "id": "EcwgoGFsqMhI",
        "outputId": "61df8701-4c0e-420e-c2ef-bb6a9420cd30"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-7-27867939b5f6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0ma\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"babad\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mj\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m   \u001b[0;32mif\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: not enough values to unpack (expected 2, got 1)"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 356
        },
        "id": "48IaX-tVzbMz",
        "outputId": "2a5b040e-8441-40cf-b516-dbc2b240e55d"
      },
      "outputs": [
        {
          "ename": "ValueError",
          "evalue": "ignored",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-33-85a9c672ecd8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mrdd2\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrdd1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"|\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfilter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m!=\u001b[0m\u001b[0mheader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m[\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m6\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mrdd3\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcus_header\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrdd2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrdd3\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollectAsMap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/pyspark/rdd.py\u001b[0m in \u001b[0;36mcollectAsMap\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   2224\u001b[0m         \u001b[0;36m4\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2225\u001b[0m         \"\"\"\n\u001b[0;32m-> 2226\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2227\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2228\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mkeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m\"RDD[Tuple[K, V]]\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;34m\"RDD[K]\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: dictionary update sequence element #0 has length 3; 2 is required"
          ]
        }
      ],
      "source": [
        "rdd1=spark.sparkContext.textFile('/content/property.csv')\n",
        "header=rdd1.map(lambda x:x.split(\"|\")).first()\n",
        "cus_header=spark.sparkContext.parallelize([['popid','size','final']])\n",
        "rdd2=rdd1.map(lambda x:x.split(\"|\")).filter(lambda x:x!=header).map(lambda x:[ x[0],x[5],str(int(x[5]) * int(x[6])) ])\n",
        "rdd3=cus_header.union(rdd2)\n",
        "for i in rdd3.collectAsMap():\n",
        "  print(i)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "kVJbmHak_Kit",
        "outputId": "b490bd10-2f4f-4007-dad6-859546bc3edc"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "100\n"
          ]
        }
      ],
      "source": [
        "df1=spark.range(1,500000).toDF(\"id1\").withColumn(\"C1\",f.lit(\"hello\"))\n",
        "df2=spark.range(200,600000).toDF(\"id2\").withColumn(\"C2\",f.lit(\"hi\"))\n",
        "df3=df1.join(df2,df1.id1==df2.id2,how=\"inner\")\n",
        "print(df3.rdd.getNumPartitions())\n",
        "df3.coals(300).write.mode(\"overwrite\").save(\"/content/check.csv\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/",
          "height": 373
        },
        "id": "cFzagTcmpU2R",
        "outputId": "fa14b1bc-8681-4ded-f43a-02c1d0b89eb4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+---+----+--------------------+\n",
            "| id|name|               funct|\n",
            "+---+----+--------------------+\n",
            "|101| aaa|[mcs, vd, education]|\n",
            "+---+----+--------------------+\n",
            "\n"
          ]
        },
        {
          "ename": "AttributeError",
          "evalue": "ignored",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-3-cf56c3e93c9f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    105\u001b[0m \u001b[0mche2\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mche\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"|\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoDF\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"id\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"name\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"study\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    106\u001b[0m \u001b[0mfdf\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mche2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwithColumn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'funct'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'study'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m','\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdrop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"study\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 107\u001b[0;31m \u001b[0mfdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    108\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    109\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'show'"
          ]
        }
      ],
      "source": [
        "# df1 = spark.read.format(\"csv\") \\\n",
        "#             .option(\"delimiter\", \"|\") \\\n",
        "#             .option(\"header\", \"true\") \\\n",
        "#             .option(\"inferSchema\", \"True\") \\\n",
        "#             .load(\"/content/oth.csv\").createOrReplaceTempView(\"hello\")\n",
        "\n",
        "# df2=df1.drop(\"_c0\")\n",
        "# df3=df2.groupBy(\"id\",\"name\").agg(f.min(df2.age).alias (\"age\") ,f.min(df2.city))\n",
        "# df3.show()\n",
        "\n",
        "# spark.sql('''select id, name,min(age) as age ,min(city) as city from hello group by id,name''').show()\n",
        "# df1 = spark.read.format(\"csv\") \\\n",
        "#             .option(\"delimiter\", \"|\") \\\n",
        "#             .option(\"header\", \"true\") \\\n",
        "#             .option(\"inferSchema\", \"True\") \\\n",
        "#             .load(\"/content/oth1.csv\")\n",
        "         \n",
        "#spark.sql('''select Name,Weight from (select Name,Weight,dense_rank() over(order by Weight desc) as rnk from hello )temp where rnk<3 ''').show()\n",
        "\n",
        "\n",
        "\n",
        "# wd=Window.orderBy(f.col(\"Weight\").desc())\n",
        "# df1.withColumn(\"rnk\",f.rank().over(wd)).filter(f.col(\"rnk\")<3).select(f.col(\"Name\"),f.col(\"Weight\")).show()\n",
        "\n",
        "# rdd1=spark.sparkContext.textFile(\"/content/oth1.csv\")\n",
        "# header = rdd1.map(lambda x:x.split('|')).first()\n",
        "# ab=rdd1.map(lambda x:x.split('|')).filter(lambda x: x!=header) \\\n",
        "#        .sortBy(lambda X:X[1],False) \\\n",
        "#        .filter(lambda x : int(x[1])>60)\n",
        "# for a in ab.collect():\n",
        "#   print(a)\n",
        "\n",
        "\n",
        "# ab=rdd1.map(lambda x:x.split('|')).filter(lambda x: x!=header) \\\n",
        "#        .sortBy(lambda X:X[1],False) \\\n",
        "#        .take(2)\n",
        "# print(ab[1])\n",
        "\n",
        "\n",
        "# df1 = spark.read.format(\"csv\") \\\n",
        "#             .option(\"delimiter\", \",\") \\\n",
        "#             .option(\"header\", \"true\") \\\n",
        "#             .option(\"inferSchema\", \"True\") \\\n",
        "#             .load(\"/content/oth2.csv\")\n",
        "# df1.withColumn(\"hob\",f.explode(f.split(f.col(\"hobbies\"),'-'))) \\\n",
        "#    .drop(f.col(\"hobbies\")).show()\n",
        "\n",
        "# rdd1=spark.sparkContext.textFile(\"/content/oth2.csv\")\n",
        "# header = rdd1.map(lambda x:x.split(',')).first()\n",
        "# ab=rdd1.map(lambda x:x.split(',')).filter(lambda x: x!=header).map(lambda x :[x[0],x[1],x[2].split(\"-\")]) \n",
        "# lis=[]\n",
        "# for a in ab.collect():\n",
        "#   for b in a[2]:\n",
        "#     lis=lis+[a[0],a[1],b]\n",
        "# print(lis)    \n",
        "\n",
        "\n",
        "\n",
        "# data = [(\"James\",\"Sales\",34), (\"Michael\",\"Sales\",56),(\"Robert\",\"Sales\",30), (\"Maria\",\"Finance\",24) ]\n",
        "# df1 = data.toDF(\"name\",\"dept\",\"age\")\n",
        "# df1.printSchema()\n",
        "\n",
        "# data2=Seq((\"James\",\"Sales\",\"NY\",9000),(\"Maria\",\"Finance\",\"CA\",9000),(\"Jen\",\"Finance\",\"NY\",7900),(\"Jeff\",\"Marketing\",\"CA\",8000))\n",
        "# df2 = data2.toDF(\"name\",\"dept\",\"state\",\"salary\")\n",
        "# df2.printSchema()\n",
        "\n",
        "\n",
        "# lis=[1,0,1,0,1,0,1,0]\n",
        "# ad=[]\n",
        "# for i in lis: \n",
        "#   if i<1:\n",
        "#     ad=ad+[i]\n",
        "# for i in lis:\n",
        "#   if i>0:\n",
        "#     ad=ad+[i]\n",
        "# print(ad)       \n",
        "\n",
        "# df2=spark.sparkContext.textFile(\"/content/file_check.csv\")\n",
        "# tt=df2.flatMap(lambda x:x.split(\" \")).map(lambda x:(x,1)).reduceByKey(lambda x,y:x+y).collectAsMap()\n",
        "# max=1\n",
        "# value=\"\"\n",
        "# for i,j in tt.items():\n",
        "#   if j < max :\n",
        "#     min=j\n",
        "# print(min)    \n",
        "# for i,j in tt.items():\n",
        "#   if j==min:\n",
        "#     print(i)\n",
        "# print(tt) \n",
        "\n",
        "# df2=spark.sparkContext.textFile(\"/content/file_check.csv\")\n",
        "# tt=df2.flatMap(lambda x:x.split(\" \")).map(lambda x:(x,1)).collect()\n",
        "# print(tt)\n",
        "\n",
        "# df2=spark.sparkContext.textFile(\"/content/check.csv\")\n",
        "# tt=df2.map(lambda x:x.split(\" \")).map(lambda x:x[0]).collect()\n",
        "# print(tt)\n",
        "\n",
        "#withColumn(“Marks”,split(col(“Marks”),”,”).cast(“array<long>”))\n",
        "from pyspark.sql.types import *\n",
        "\n",
        "#df1=spark.read.format(\"csv\").option(\"delimiter\",\"|\").option(\"Header\",True).load(\"/content/file_explode.csv\")\n",
        "#df1.withColumn(\"chdc\",f.split(f.col(\"education\"),\",\").cast(\"array<long>\")).show()\n",
        "che=spark.sparkContext.parallelize([\"101|aaa|mcs,vd,education\"])\n",
        "che2=che.map(lambda x: (x.split(\"|\") )).toDF([\"id\",\"name\",\"study\"])\n",
        "fdf=che2.withColumn('funct', f.split('study',',')).drop(\"study\").show()\n",
        "fdf.select(f.col(\"id\"),f.explode(f.col(\"funct\"))).show()\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6Zfp-MT2izp9"
      },
      "source": [
        "SESSION_Allocation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KpY9O6lqxxHY",
        "outputId": "c31be911-6958-4bc3-9b22-207fee97be52"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+---+-------------------+-------------------+------------------+------------------+----------------+-------------------+---------------+-----+\n",
            "|idt|dtime              |prev_dtime         |session_diff_30min|session_flag_30min|session_id_30min|min_dtime          |session_id_2hrs|total|\n",
            "+---+-------------------+-------------------+------------------+------------------+----------------+-------------------+---------------+-----+\n",
            "|U1 |2020-03-28 03:45:00|null               |null              |1                 |1               |2020-03-28 03:45:00|0              |1    |\n",
            "|U1 |2020-03-28 04:10:00|2020-03-28 03:45:00|25.0              |0                 |1               |2020-03-28 03:45:00|0              |1    |\n",
            "|U1 |2020-03-28 04:35:00|2020-03-28 04:10:00|25.0              |0                 |1               |2020-03-28 03:45:00|0              |1    |\n",
            "|U1 |2020-03-28 04:55:00|2020-03-28 04:35:00|20.0              |0                 |1               |2020-03-28 03:45:00|0              |1    |\n",
            "|U1 |2020-03-28 05:20:00|2020-03-28 04:55:00|25.0              |0                 |1               |2020-03-28 03:45:00|0              |1    |\n",
            "|U1 |2020-03-28 05:45:00|2020-03-28 05:20:00|25.0              |0                 |1               |2020-03-28 03:45:00|1              |2    |\n",
            "|U1 |2020-03-28 06:10:00|2020-03-28 05:45:00|25.0              |0                 |1               |2020-03-28 03:45:00|1              |2    |\n",
            "|U1 |2020-03-28 06:58:00|2020-03-28 06:10:00|48.0              |1                 |2               |2020-03-28 03:45:00|1              |3    |\n",
            "|U1 |2020-03-28 07:15:00|2020-03-28 06:58:00|17.0              |0                 |2               |2020-03-28 03:45:00|1              |3    |\n",
            "|U1 |2020-03-28 07:25:00|2020-03-28 07:15:00|10.0              |0                 |2               |2020-03-28 03:45:00|1              |3    |\n",
            "|U1 |2020-03-28 07:30:00|2020-03-28 07:25:00|5.0               |0                 |2               |2020-03-28 03:45:00|1              |3    |\n",
            "|U1 |2020-03-28 09:35:00|2020-03-28 07:30:00|125.0             |1                 |3               |2020-03-28 03:45:00|1              |4    |\n",
            "|U1 |2020-03-28 10:00:00|2020-03-28 09:35:00|25.0              |0                 |3               |2020-03-28 03:45:00|1              |4    |\n",
            "|U1 |2020-03-28 10:10:00|2020-03-28 10:00:00|10.0              |0                 |3               |2020-03-28 03:45:00|1              |4    |\n",
            "|U1 |2020-03-28 12:15:00|2020-03-28 10:10:00|125.0             |1                 |4               |2020-03-28 03:45:00|1              |5    |\n",
            "|U2 |2020-03-28 09:35:00|null               |null              |1                 |1               |2020-03-28 09:35:00|0              |1    |\n",
            "|U2 |2020-03-28 10:00:00|2020-03-28 09:35:00|25.0              |0                 |1               |2020-03-28 09:35:00|0              |1    |\n",
            "|U2 |2020-03-28 10:40:00|2020-03-28 10:00:00|40.0              |1                 |2               |2020-03-28 09:35:00|0              |2    |\n",
            "+---+-------------------+-------------------+------------------+------------------+----------------+-------------------+---------------+-----+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "\n",
        "df1 = spark.read.format(\"csv\") \\\n",
        "            .option(\"delimiter\", \",\") \\\n",
        "            .option(\"header\", \"true\") \\\n",
        "            .option(\"TimeStampFormat\", \"yyyy-MM-dd HH:mm\") \\\n",
        "            .option(\"inferSchema\", \"True\") \\\n",
        "            .load(\"/content/session2.csv\")\n",
        "\n",
        "# populate prev dtime for each record\n",
        "df_30 = df1.withColumn(\"prev_dtime\",f.lag(\"dtime\").over(Window.partitionBy(\"idt\").orderBy(\"dtime\")),)\n",
        "\n",
        "#find the diff of dtime and prev_dtime if it is less than or equal to 30 min add 0 else add 1\n",
        "df_30 = df_30.withColumn(\"session_diff_30min\", \\\n",
        "                   (f.col(\"dtime\").cast(LongType()) - f.col(\"prev_dtime\").cast(LongType()))/60)\n",
        "df_30 = df_30.withColumn(\"session_flag_30min\", \\\n",
        "                  f.when(f.col(\"session_diff_30min\")<30,0).otherwise(1))\n",
        "\n",
        "df_30 = df_30.withColumn(\"session_id_30min\",f.sum(\"session_flag_30min\").over(Window.partitionBy(\"idt\").orderBy(\"dtime\")))\n",
        "\n",
        "#df_30_mindt = df_30.groupBy(\"idt\", \"session_id_30min\").agg(f.min(f.col(\"dtime\")).alias(\"min_dtime\"))\n",
        "df_30_mindt = df_30.groupBy(\"idt\").agg(f.min(f.col(\"dtime\")).alias(\"min_dtime\"))\n",
        "\n",
        "df_2hrs_1 = df_30.join(df_30_mindt, [\"idt\" ], \"inner\")\n",
        "df_2hrs_1=df_2hrs_1.withColumn(\"session_id_2hrs\", \\\n",
        "                   f.when(f.floor((f.col(\"dtime\").cast(LongType()) - f.col(\"min_dtime\").cast(LongType()))/7200)>=1,1).otherwise(0))\n",
        "\n",
        "df_2hrs_1=df_2hrs_1.withColumn(\"total\",f.col(\"session_id_2hrs\")+f.col(\"session_id_30min\"))\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# create the session id using the running total\n",
        "# df_30 = df_30.withColumn(\"session_id_30min\",f.sum(\"session_flag_30min\").over(Window.partitionBy(\"idt\").orderBy(\"dtime\")))\n",
        "# df_30.show(truncate=False)\n",
        "\n",
        "#now we have completed the creating the session id with 30 min logic\n",
        "#start implement the 2hrs logic\n",
        "#for each user and session id get the minimum date time\n",
        "# df_30_mindt = df_30.groupBy(\"idt\", \"session_id_30min\").agg(f.min(f.col(\"dtime\")).alias(\"min_dtime\"))\n",
        "# df_30_mindt.show(truncate=False)\n",
        "\n",
        "#now join df_30 with df_30mindt to populate the min_dtime column\n",
        "# df_2hrs_1 = df_30.join(df_30_mindt, [\"idt\" , \"session_id_30min\"], \"inner\")\n",
        "\n",
        "# #now find the differeent between dtime and min_dtime if the difference is less than 7200 (2HRS) add 0 else 1\n",
        "# df_2hrs_2 = df_2hrs_1.withColumn(\"session_flag_2hrs\", \\\n",
        "#         f.when(f.col(\"dtime\").cast(LongType()) - f.col(\"min_dtime\").cast(LongType()) <= 7200, 0).otherwise(1))\n",
        "\n",
        "\n",
        "# df_2hrs_2 = df_2hrs_2.withColumn(\"session_flag_2hrs\",f.sum(\"session_flag_2hrs\").over(Window.partitionBy(\"idt\").orderBy(\"dtime\")))\n",
        "\n",
        "\n",
        "# #now to populate the final session id sum session_flag_30min and session_flag_2hrs\n",
        "#df_final = df_2hrs_1.withColumn(\"total\", f.col(\"session_id_30min\") + f.col(\"session_id_2hrs\"))\n",
        "\n",
        "df_2hrs_1.show(truncate=False)\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E4AUbuyW7Mk1"
      },
      "source": [
        "Fuel_theft\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ijnRcDpfnyvs",
        "outputId": "93d28803-4e92-4762-97b2-dbfc8147f3fb"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+--------+-------------------+----------+--------+----+-------+\n",
            "| trip_id|          timestamp|fuel_level|previous|diff|   flag|\n",
            "+--------+-------------------+----------+--------+----+-------+\n",
            "|trip_006|2020-03-28 12:18:00|    280.49|  281.74|   1|incline|\n",
            "|trip_006|2020-03-28 12:23:00|    272.49|  280.49|   8|incline|\n",
            "|trip_006|2020-03-28 14:53:00|    455.22|  255.22|-201|  Theft|\n",
            "+--------+-------------------+----------+--------+----+-------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "df1 = spark.read.format(\"csv\") \\\n",
        "            .option(\"delimiter\", \",\") \\\n",
        "            .option(\"header\", \"true\") \\\n",
        "            .option(\"TimeStampFormat\", \"yyyy-MM-dd HH:mm\") \\\n",
        "            .option(\"inferSchema\", \"True\") \\\n",
        "            .load(\"/content/fuel.csv\")\n",
        "\n",
        "df2=df1.withColumn(\"previous\"     ,f.lag(\"fuel_level\").over(Window.orderBy(\"timestamp\"))) \\\n",
        "       .withColumn(\"diff\"         ,f.floor(f.col(\"previous\")-f.col(\"fuel_level\"))) \n",
        "df2=df2.withColumn(\"flag_temp\",f.lit(\"Normal\"))\n",
        "\n",
        "mvalue=df2.filter(f.col(\"diff\")>1).rdd.map(lambda x: x.asDict()).collect()\n",
        "mdict ={i['previous']:i for i in mvalue}\n",
        "Fuel_list=df2.select(\"fuel_level\").rdd.flatMap(lambda x: x).collect()\n",
        "\n",
        "def flag_convert_fun(a,b):\n",
        "  flag=\"Normal\"\n",
        "  if str(a)==str(b):\n",
        "    for i in Fuel_list:\n",
        "      if math.floor(a-i)==1:\n",
        "        flag=\"incline\"\n",
        "  return flag\n",
        "\n",
        "flag_convert=f.udf(lambda z,b:flag_convert_fun(z,b))\n",
        "for i in mdict:\n",
        "  df3=df2.withColumn(\"flag\",f.when((flag_convert(f.col(\"fuel_level\"),f.lit(i))==\"incline\") \\\n",
        "                      & (f.col(\"flag_temp\")==\"Normal\"),\"incline\").otherwise(f.col(\"flag_temp\")))\n",
        "  df2=df3.withColumn(\"flag_temp\",f.col(\"flag\"))\n",
        "\n",
        "for i in mdict:\n",
        "  df3=df2.withColumn(\"flag\",f.when((f.col(\"fuel_level\")==i)&(f.col(\"flag_temp\")==\"Normal\"),\"Theft\") \\\n",
        "                     .otherwise(f.col(\"flag_temp\")))\n",
        "df3.drop(f.col(\"flag_temp\")).filter(f.col(\"flag\")!=\"Normal\").show()\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "UPSERTLOGIC "
      ],
      "metadata": {
        "id": "Ute8aGPbrgXy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8z92yUS12GbK",
        "outputId": "da6eae8d-455d-414c-bbdd-85f00e06ba7d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "inc ['ID', 'NAMES', 'COUNTRY', 'DATE', 'audit_time']\n",
            "hist ['ID', 'NAMES', 'COUNTRY', 'DATE', 'audit_time']\n",
            "+---+---------------+-----------+--------------------+--------------------+\n",
            "| ID|          NAMES|    COUNTRY|                DATE|          audit_time|\n",
            "+---+---------------+-----------+--------------------+--------------------+\n",
            "|  7| Ferris Knowles|New Zealand|2023-05-17T00:00:...|2023-01-13T12:34:...|\n",
            "|  8|Barclay Wheeler|   Pakistan|2023-05-17T00:00:...|2023-01-13T12:34:...|\n",
            "|  9|updEthan Hodges|     Turkey|2023-05-11T00:00:...|2023-01-13 12:34:...|\n",
            "| 12|    Judah Hayes|    Germany|2023-05-21T00:00:...|2023-01-13T12:34:...|\n",
            "| 29|   Ethan Hodges|     Turkey|2023-05-11T00:00:...|2023-01-13 12:34:...|\n",
            "+---+---------------+-----------+--------------------+--------------------+\n",
            "\n",
            "inc ['ID', 'NAMES', 'COUNTRY', 'DATE', 'audit_time']\n",
            "hist ['ID', 'NAMES', 'COUNTRY', 'DATE', 'audit_time']\n",
            "+---+----------------+-------+--------------------+--------------------+\n",
            "| ID|           NAMES|COUNTRY|                DATE|          audit_time|\n",
            "+---+----------------+-------+--------------------+--------------------+\n",
            "|  5|updDillon Garcia|  India|2022-11-14T00:00:...|2023-01-13 12:34:...|\n",
            "| 32|   Dillon Garcia|  India|2022-11-14T00:00:...|2023-01-13 12:34:...|\n",
            "+---+----------------+-------+--------------------+--------------------+\n",
            "\n",
            "inc ['ID', 'NAMES', 'COUNTRY', 'DATE', 'audit_time']\n",
            "hist ['ID', 'NAMES', 'COUNTRY', 'DATE', 'audit_time']\n",
            "+---+----------------+-------------+--------------------+--------------------+\n",
            "| ID|           NAMES|      COUNTRY|                DATE|          audit_time|\n",
            "+---+----------------+-------------+--------------------+--------------------+\n",
            "|  3|    Aladdin Buck|  Netherlands|2022-01-31T00:00:...|2023-01-13T12:34:...|\n",
            "| 13|updNolan Ferrell|United States|2022-01-27T00:00:...|2023-01-13 12:34:...|\n",
            "| 33|   Nolan Ferrell|United States|2022-01-27T00:00:...|2023-01-13 12:34:...|\n",
            "+---+----------------+-------------+--------------------+--------------------+\n",
            "\n",
            "inc ['ID', 'NAMES', 'COUNTRY', 'DATE', 'audit_time']\n",
            "hist ['ID', 'NAMES', 'COUNTRY', 'DATE', 'audit_time']\n",
            "+---+---------------+-------+--------------------+--------------------+\n",
            "| ID|          NAMES|COUNTRY|                DATE|          audit_time|\n",
            "+---+---------------+-------+--------------------+--------------------+\n",
            "| 22|updWilla Cooper| France|2023-02-27T00:00:...|2023-01-13 12:34:...|\n",
            "| 31|   Willa Cooper| France|2023-02-27T00:00:...|2023-01-13 12:34:...|\n",
            "+---+---------------+-------+--------------------+--------------------+\n",
            "\n",
            "inc ['ID', 'NAMES', 'COUNTRY', 'DATE', 'audit_time']\n",
            "hist ['ID', 'NAMES', 'COUNTRY', 'DATE', 'audit_time']\n",
            "+---+-------------------+-------------+--------------------+--------------------+\n",
            "| ID|              NAMES|      COUNTRY|                DATE|          audit_time|\n",
            "+---+-------------------+-------------+--------------------+--------------------+\n",
            "|  5|      Ora Blackwell|United States|2023-03-31T00:00:...|2023-01-13T12:34:...|\n",
            "|  8|updChelsea Phillips|       Sweden|2023-03-23T00:00:...|2023-01-13 12:34:...|\n",
            "| 30|   Chelsea Phillips|       Sweden|2023-03-23T00:00:...|2023-01-13 12:34:...|\n",
            "+---+-------------------+-------------+--------------------+--------------------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "from pyspark.sql.functions import *\n",
        "\n",
        "df1 = spark.read.format(\"csv\") \\\n",
        "            .option(\"delimiter\", \",\") \\\n",
        "            .option(\"header\", \"true\") \\\n",
        "            .option(\"TimeStampFormat\", \"yyyy-MM-dd HH:mm\") \\\n",
        "            .option(\"inferSchema\", \"True\") \\\n",
        "            .load(\"/content/HIST.csv\")\n",
        "df1=df1.withColumn(\"audit_time\",current_timestamp())\n",
        "df1=df1.withColumn(\"audit_col\",f.concat(f.year(f.col(\"Date\")),f.month(f.col(\"Date\"))))  \n",
        "df1.write.mode(\"overwrite\").partitionBy(\"audit_col\").option(\"Header\",True).format(\"csv\").save(\"/content/datapart\")   \n",
        "      \n",
        "\n",
        "\n",
        "\n",
        "\n",
        "df2 = spark.read.format(\"csv\") \\\n",
        "            .option(\"delimiter\", \",\") \\\n",
        "            .option(\"header\", \"true\") \\\n",
        "            .option(\"TimeStampFormat\", \"yyyy-MM-dd\") \\\n",
        "            .option(\"inferSchema\", \"True\") \\\n",
        "            .load(\"/content/INC.csv\")\n",
        "df2=df2.withColumn(\"audit_time\",current_timestamp())\n",
        "df2=df2.withColumn(\"audit_col\",f.concat(f.year(f.col(\"Date\")),f.month(f.col(\"Date\"))))  \n",
        "\n",
        "\n",
        "\n",
        "\n",
        "datecnt=df2.select(f.col(\"audit_col\"))\n",
        "a=set([i[0] for i in datecnt.rdd.collect()])\n",
        "for i in a:\n",
        "  df_main = spark.read.format(\"csv\") \\\n",
        "            .option(\"delimiter\", \",\") \\\n",
        "            .option(\"header\", \"true\") \\\n",
        "            .option(\"TimeStampFormat\", \"yyyy-MM-dd\") \\\n",
        "            .option(\"inferSchema\", \"True\") \\\n",
        "            .load(\"/content/datapart/audit_col=\"+i)\n",
        "\n",
        "  df_inc= df2.filter(f.col(\"audit_col\")==i).drop(\"audit_col\")\n",
        "  print(\"inc\",df_inc.columns)\n",
        "  print(\"hist\",df_main.columns)\n",
        "  df_final=df_main.unionByName(df_inc).withColumn(\"rnk\",row_number().over(Window.partitionBy(\"ID\") \\\n",
        "                                      .orderBy(f.col(\"audit_time\").asc()))).filter(\"rnk=1\") \\\n",
        "                                      .drop(\"rnk\")\n",
        "  df_final.write.mode(\"overwrite\").option(\"header\",True).format(\"csv\").save(\"/content/data_temp\")      \n",
        "  df_temp=spark.read.format(\"csv\").option(\"delimiter\", \",\").option(\"header\", \"true\").load(\"/content/data_temp\")\n",
        "  df_temp.write.mode(\"overwrite\").option(\"Header\",True).format(\"csv\").save(\"/content/datapart/audit_col=\"+i)   \n",
        "  df_temp.show()                                                      \n",
        "   \n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# inc ['ID', 'NAMES', 'COUNTRY', 'DATE', 'audit_time']\n",
        "# hist ['ID', 'NAMES', 'COUNTRY', 'DATE', 'audit_time']\n",
        "# ['ID', 'NAMES', 'COUNTRY', 'DATE', 'audit_time', 'audit_col']\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df=spark.read.format(\"csv\").option(\"header\",True).option(\"InferSchema\",True).load(\"/content/datapart/\").createOrReplaceTempView(\"temp\")\n",
        "spark.sql(\"select * from temp order by DATE \").show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3JZ75-x3H_U8",
        "outputId": "8e54fba0-c04e-4f76-ac24-8af0834860c1"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---+------------------+------------------+-------------------+--------------------+---------+\n",
            "| ID|             NAMES|           COUNTRY|               DATE|          audit_time|audit_col|\n",
            "+---+------------------+------------------+-------------------+--------------------+---------+\n",
            "| 13|  updNolan Ferrell|     United States|2022-01-27 00:00:00|2023-01-13 12:34:...|    20221|\n",
            "| 33|     Nolan Ferrell|     United States|2022-01-27 00:00:00|2023-01-13 12:34:...|    20221|\n",
            "|  3|      Aladdin Buck|       Netherlands|2022-01-31 00:00:00|2023-01-13 12:34:...|    20221|\n",
            "| 21|      Deirdre Bird|        Costa Rica|2022-03-18 00:00:00|2023-01-13 12:34:...|    20223|\n",
            "| 17|       Dai Bernard|      South Africa|2022-05-05 00:00:00|2023-01-13 12:34:...|    20225|\n",
            "|  2|       Carol Gross|           Nigeria|2022-05-06 00:00:00|2023-01-13 12:34:...|    20225|\n",
            "| 18|         Jolie Ray|         Australia|2022-05-22 00:00:00|2023-01-13 12:34:...|    20225|\n",
            "| 22|      Ivan Puckett|           Nigeria|2022-06-07 00:00:00|2023-01-13 12:34:...|    20226|\n",
            "| 18|       Finn Travis|Russian Federation|2022-07-03 00:00:00|2023-01-13 12:34:...|    20227|\n",
            "| 15|Gwendolyn Callahan|            Brazil|2022-07-13 00:00:00|2023-01-13 12:34:...|    20227|\n",
            "|  5| Xanthus Hendricks|           Germany|2022-08-11 00:00:00|2023-01-13 12:34:...|    20228|\n",
            "|  1|    Bruno Marshall|           Belgium|2022-08-26 00:00:00|2023-01-13 12:34:...|    20228|\n",
            "|  9|  Callie Cervantes|       Netherlands|2022-10-13 00:00:00|2023-01-13 12:34:...|   202210|\n",
            "| 17|    Nolan Erickson|         Singapore|2022-10-15 00:00:00|2023-01-13 12:34:...|   202210|\n",
            "|  5|  updDillon Garcia|             India|2022-11-14 00:00:00|2023-01-13 12:34:...|   202211|\n",
            "| 32|     Dillon Garcia|             India|2022-11-14 00:00:00|2023-01-13 12:34:...|   202211|\n",
            "|  3|  Demetria Freeman|              Peru|2023-01-26 00:00:00|2023-01-13 12:34:...|    20231|\n",
            "|  4|   Pandora O'brien|           Ukraine|2023-01-30 00:00:00|2023-01-13 12:34:...|    20231|\n",
            "| 31|      Willa Cooper|            France|2023-02-27 00:00:00|2023-01-13 12:34:...|    20232|\n",
            "| 22|   updWilla Cooper|            France|2023-02-27 00:00:00|2023-01-13 12:34:...|    20232|\n",
            "+---+------------------+------------------+-------------------+--------------------+---------+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "60drpsbe7Yzx"
      },
      "source": [
        "SQL_Test\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/",
          "height": 608
        },
        "id": "BUIyskNRueFY",
        "outputId": "209dc075-adb3-409b-8370-3817d33d6088"
      },
      "outputs": [
        {
          "ename": "ParseException",
          "evalue": "ignored",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mParseException\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-14-3188b8a82927>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mspark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"csv\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moption\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"header\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moption\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"InferSchema\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"/content/EMP_data.csv\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreateOrReplaceTempView\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"mtable\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mspark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msql\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"with mcheck as (select  first_name,last_name,email,gender,ip_address, row_number() over( partition by first_name,last_name,email,gender,ip_address order by first_name,last_name,email,gender,ip_address )as rnk from mtable ) insert overwrite into mtable select * from mcheck where rnk=1\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/pyspark/sql/session.py\u001b[0m in \u001b[0;36msql\u001b[0;34m(self, sqlQuery, **kwargs)\u001b[0m\n\u001b[1;32m   1032\u001b[0m             \u001b[0msqlQuery\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mformatter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msqlQuery\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1033\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1034\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jsparkSession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msql\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msqlQuery\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1035\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1036\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1319\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1320\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1321\u001b[0;31m         return_value = get_return_value(\n\u001b[0m\u001b[1;32m   1322\u001b[0m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[1;32m   1323\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    194\u001b[0m                 \u001b[0;31m# Hide where the exception came from that shows a non-Pythonic\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    195\u001b[0m                 \u001b[0;31m# JVM exception message.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 196\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mconverted\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    197\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    198\u001b[0m                 \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mParseException\u001b[0m: \nSyntax error at or near 'mtable': extra input 'mtable'(line 1, pos 245)\n\n== SQL ==\nwith mcheck as (select  first_name,last_name,email,gender,ip_address, row_number() over( partition by first_name,last_name,email,gender,ip_address order by first_name,last_name,email,gender,ip_address )as rnk from mtable ) insert overwrite into mtable select * from mcheck where rnk=1\n-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------^^^\n"
          ]
        }
      ],
      "source": [
        "df=spark.read.format(\"csv\").option(\"header\",True).option(\"InferSchema\",True).load(\"/content/datapart/\")\n",
        "spark.sql(\"with mcheck as (select  first_name,last_name,email,gender,ip_address, row_number() over( partition by first_name,last_name,email,gender,ip_address order by first_name,last_name,email,gender,ip_address )as rnk from mtable ) insert overwrite into mtable select * from mcheck where rnk=1\").show()\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sjpaR3xuXGOG",
        "outputId": "36327595-9f66-41cf-902d-28b2cc4ba14d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+---+-----------+---------+\n",
            "|idf|sum(amount)|occurance|\n",
            "+---+-----------+---------+\n",
            "|101|      600.0|        2|\n",
            "+---+-----------+---------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "df_1=spark.read.format(\"csv\").option(\"header\",True).load(\"/content/check_22.csv\")\n",
        "df=df_1.createOrReplaceTempView(\"check7\")\n",
        "fg=spark.sql('''select idf,sum(amount),count(idf) as occurance from check7 where idf in \n",
        "(select i.idf from check7 as i group by idf having count( distinct orderdate) >1)\n",
        " group by idf ''').createOrReplaceTempView(\"check77\")\n",
        "gg=spark.sql('''select * from check77''').show()\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yYkzlprDitGa",
        "outputId": "15360bdb-2c52-4000-b3e7-5cb8b609d2ed"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[Row(r='1a2s3ae')]\n"
          ]
        }
      ],
      "source": [
        "df=spark.createDataFrame([('translate',)], ['a']).select(F.translate('a', \"rnlt\", \"123\").alias('r')).collect()\n",
        "print(df)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FF_GSlViisvD"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 201
        },
        "id": "iataIps8iHFi",
        "outputId": "504773bf-f4ee-41ea-ab03-93f181535cb8"
      },
      "outputs": [
        {
          "ename": "NameError",
          "evalue": "ignored",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-23-f75a89c0ba8f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mdF\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mspark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreateDataFrame\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mRow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mintlist\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmapfield\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m\"a\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m\"b\"\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mselect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexplode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mintlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0malias\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"anInt\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'Row' is not defined"
          ]
        }
      ],
      "source": [
        "dF = spark.createDataFrame\n",
        "([Row(a=1, intlist=[1,2,3], mapfield={\"a\": \"b\"})])\n",
        "df.select(F.explode(df.intlist).alias(\"anInt\")).collect()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PEYd9VnKiG5J"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 363
        },
        "id": "MQDQ9Pwb0p51",
        "outputId": "8d4d0a7b-2de8-4eae-ea0f-e71f93575215"
      },
      "outputs": [
        {
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-4-cc3a00ad19cf>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mpyspark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msql\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mSparkSession\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mspark\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSparkSession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuilder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmaster\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"local[*]\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m                     \u001b[0;34m.\u001b[0m\u001b[0mappName\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Test_spark'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m                     \u001b[0;34m.\u001b[0m\u001b[0mgetOrCreate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/pyspark/sql/session.py\u001b[0m in \u001b[0;36mgetOrCreate\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    267\u001b[0m                         \u001b[0msparkConf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    268\u001b[0m                     \u001b[0;31m# This SparkContext may be an existing one.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 269\u001b[0;31m                     \u001b[0msc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSparkContext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetOrCreate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msparkConf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    270\u001b[0m                     \u001b[0;31m# Do not update `SparkConf` for existing `SparkContext`, as it's shared\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    271\u001b[0m                     \u001b[0;31m# by all sessions.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/pyspark/context.py\u001b[0m in \u001b[0;36mgetOrCreate\u001b[0;34m(cls, conf)\u001b[0m\n\u001b[1;32m    481\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mSparkContext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    482\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mSparkContext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_active_spark_context\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 483\u001b[0;31m                 \u001b[0mSparkContext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconf\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconf\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mSparkConf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    484\u001b[0m             \u001b[0;32massert\u001b[0m \u001b[0mSparkContext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_active_spark_context\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    485\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mSparkContext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_active_spark_context\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/pyspark/context.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, master, appName, sparkHome, pyFiles, environment, batchSize, serializer, conf, gateway, jsc, profiler_cls, udf_profiler_cls)\u001b[0m\n\u001b[1;32m    193\u001b[0m             )\n\u001b[1;32m    194\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 195\u001b[0;31m         \u001b[0mSparkContext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_ensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgateway\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mgateway\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconf\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    196\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    197\u001b[0m             self._do_init(\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/pyspark/context.py\u001b[0m in \u001b[0;36m_ensure_initialized\u001b[0;34m(cls, instance, gateway, conf)\u001b[0m\n\u001b[1;32m    415\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mSparkContext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    416\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mSparkContext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_gateway\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 417\u001b[0;31m                 \u001b[0mSparkContext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_gateway\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgateway\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mlaunch_gateway\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    418\u001b[0m                 \u001b[0mSparkContext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jvm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSparkContext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_gateway\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjvm\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    419\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/pyspark/java_gateway.py\u001b[0m in \u001b[0;36mlaunch_gateway\u001b[0;34m(conf, popen_kwargs)\u001b[0m\n\u001b[1;32m    101\u001b[0m             \u001b[0;31m# Wait for the file to appear, or for the process to exit, whichever happens first.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    102\u001b[0m             \u001b[0;32mwhile\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mproc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpoll\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misfile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconn_info_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 103\u001b[0;31m                 \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msleep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0.1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    104\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    105\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misfile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconn_info_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "from pyspark.sql import SparkSession\n",
        "spark = SparkSession.builder.master(\"local[*]\") \\\n",
        "                    .appName('Test_spark') \\\n",
        "                    .getOrCreate()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 105
        },
        "id": "3spwYnP29klD",
        "outputId": "7b409adc-8eb4-4579-f7d8-2618b0315d86"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "H\n",
            "ello\n",
            "W\n",
            "orld\n"
          ]
        },
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'Hello World '"
            ]
          },
          "execution_count": 13,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "def convertCase(str):\n",
        "    resStr=\"\"\n",
        "    arr = str.split(\" \")\n",
        "    for x in arr:\n",
        "       print (x[0:1].upper())\n",
        "       print( x[1:len(x)])\n",
        "       resStr= resStr + x[0:1].upper() + x[1:len(x)] + \" \"\n",
        "       \n",
        "    return resStr\n",
        "\n",
        "convertCase(\"hello world\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7XAtePFv3B2Y"
      },
      "outputs": [],
      "source": [
        "import pyspark\n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.functions import col, udf\n",
        "from pyspark.sql.types import StringType\n",
        "\n",
        "spark = SparkSession.builder.appName('SparkByExamples.com').getOrCreate()\n",
        "\n",
        "columns = [\"Seqno\",\"Name\"]\n",
        "data = [(\"1\", \"john jones\"),\n",
        "    (\"2\", \"tracey smith\"),\n",
        "    (\"3\", \"amy sanders\")]\n",
        "\n",
        "df = spark.createDataFrame(data=data,schema=columns)\n",
        "\n",
        "df.show(truncate=False)\n",
        "\n",
        "def convertCase(str):\n",
        "    resStr=\"\"\n",
        "    arr = str.split(\" \")\n",
        "    for x in arr:\n",
        "       resStr= resStr + x[0:1].upper() + x[1:len(x)] + \" \"\n",
        "    return resStr \n",
        "\n",
        "\"\"\" Converting function to UDF \"\"\"\n",
        "convertUDF = udf(lambda z: convertCase(z))\n",
        "\n",
        "df.select(col(\"Seqno\"), \\\n",
        "    convertUDF(col(\"Name\")).alias(\"Name\") ) \\\n",
        ".show(truncate=False)\n",
        "\n",
        "def upperCase(str):\n",
        "    return str.upper()\n",
        "\n",
        "upperCaseUDF = udf(lambda z:upperCase(z),StringType())    \n",
        "\n",
        "df.withColumn(\"Cureated Name\", upperCaseUDF(col(\"Name\"))) \\\n",
        ".show(truncate=False)\n",
        "\n",
        "\"\"\" Using UDF on SQL \"\"\"\n",
        "spark.udf.register(\"convertUDF\", convertCase,StringType())\n",
        "df.createOrReplaceTempView(\"NAME_TABLE\")\n",
        "spark.sql(\"select Seqno, convertUDF(Name) as Name from NAME_TABLE\") \\\n",
        "     .show(truncate=False)\n",
        "     \n",
        "spark.sql(\"select Seqno, convertUDF(Name) as Name from NAME_TABLE \" + \\\n",
        "          \"where Name is not null and convertUDF(Name) like '%John%'\") \\\n",
        "     .show(truncate=False)  \n",
        "     \n",
        "\"\"\" null check \"\"\"\n",
        "\n",
        "columns = [\"Seqno\",\"Name\"]\n",
        "data = [(\"1\", \"john jones\"),\n",
        "    (\"2\", \"tracey smith\"),\n",
        "    (\"3\", \"amy sanders\"),\n",
        "    ('4',None)]\n",
        "\n",
        "df2 = spark.createDataFrame(data=data,schema=columns)\n",
        "df2.show(truncate=False)\n",
        "df2.createOrReplaceTempView(\"NAME_TABLE2\")\n",
        "    \n",
        "spark.udf.register(\"_nullsafeUDF\", lambda str: convertCase(str) if not str is None else \"\" , StringType())\n",
        "\n",
        "spark.sql(\"select _nullsafeUDF(Name) from NAME_TABLE2\") \\\n",
        "     .show(truncate=False)\n",
        "\n",
        "spark.sql(\"select Seqno, _nullsafeUDF(Name) as Name from NAME_TABLE2 \" + \\\n",
        "          \" where Name is not null and _nullsafeUDF(Name) like '%John%'\") \\\n",
        "     .show(truncate=False)  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IL93kXddYGbi",
        "outputId": "c28881f7-61f6-47e4-bf75-a79d2f22979d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+-----+------------+\n",
            "|Seqno|Name        |\n",
            "+-----+------------+\n",
            "|1    |john jones  |\n",
            "|2    |tracey smith|\n",
            "|3    |amy sanders |\n",
            "|4    |null        |\n",
            "+-----+------------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "\n",
        "import pyspark\n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.functions import col, udf\n",
        "from pyspark.sql import functions as F\n",
        "from pyspark.sql.types import StringType\n",
        "\n",
        "spark = SparkSession.builder.appName('SparkByExamples.com').getOrCreate()\n",
        "columns = [\"Seqno\",\"Name\"]\n",
        "data = [(\"1\", \"john jones\"),\n",
        "    (\"2\", \"tracey smith\"),\n",
        "    (\"3\", \"amy sanders\"),\n",
        "    ('4',None)]\n",
        "\n",
        "df2 = spark.createDataFrame(data=data,schema=columns)\n",
        "df2.show(truncate=False)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 311
        },
        "id": "fsCwHGnRYKzY",
        "outputId": "df3f9624-5047-486b-80b7-0ebee476c135"
      },
      "outputs": [
        {
          "ename": "TypeError",
          "evalue": "ignored",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-24-07d8a00369f9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdf2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mselect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mregexp_replace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcol\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m' '\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m''\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0malias\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mregexp_replace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcname\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m' '\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m''\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mcname\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdf2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-24-07d8a00369f9>\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdf2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mselect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mregexp_replace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcol\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m' '\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m''\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0malias\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mregexp_replace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcname\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m' '\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m''\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mcname\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdf2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pyspark/sql/column.py\u001b[0m in \u001b[0;36malias\u001b[0;34m(self, *alias, **kwargs)\u001b[0m\n\u001b[1;32m    881\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mColumn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"as\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0malias\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mjmeta\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    882\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 883\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mColumn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"as\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0malias\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    884\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    885\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mmetadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1311\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1312\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1313\u001b[0;31m         \u001b[0margs_command\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtemp_args\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_build_args\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1314\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1315\u001b[0m         \u001b[0mcommand\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mproto\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCALL_COMMAND_NAME\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/py4j/java_gateway.py\u001b[0m in \u001b[0;36m_build_args\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1275\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_build_args\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1276\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconverters\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconverters\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1277\u001b[0;31m             \u001b[0;34m(\u001b[0m\u001b[0mnew_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_args\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1278\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1279\u001b[0m             \u001b[0mnew_args\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/py4j/java_gateway.py\u001b[0m in \u001b[0;36m_get_args\u001b[0;34m(self, args)\u001b[0m\n\u001b[1;32m   1262\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0mconverter\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconverters\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1263\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0mconverter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcan_convert\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1264\u001b[0;31m                         \u001b[0mtemp_arg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconverter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconvert\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1265\u001b[0m                         \u001b[0mtemp_args\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtemp_arg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1266\u001b[0m                         \u001b[0mnew_args\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtemp_arg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/py4j/java_collections.py\u001b[0m in \u001b[0;36mconvert\u001b[0;34m(self, object, gateway_client)\u001b[0m\n\u001b[1;32m    508\u001b[0m         \u001b[0mArrayList\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mJavaClass\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"java.util.ArrayList\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgateway_client\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    509\u001b[0m         \u001b[0mjava_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mArrayList\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 510\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0melement\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mobject\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    511\u001b[0m             \u001b[0mjava_list\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0melement\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    512\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mjava_list\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pyspark/sql/column.py\u001b[0m in \u001b[0;36m__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    558\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    559\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__iter__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 560\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Column is not iterable\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    561\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    562\u001b[0m     \u001b[0;31m# string methods\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: Column is not iterable"
          ]
        }
      ],
      "source": [
        "df2.select([F.regexp_replace(F.col(cname),' ','').alias (F.regexp_replace(cname,' ','')) for cname in df2.columns]).show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LiMq_FoGZ2bS"
      },
      "outputs": [],
      "source": [
        "# prime number testing\n",
        "Last_number=12\n",
        "sum=0\n",
        "\n",
        "for number in range(2, Last_number + 1):\n",
        "    print(\"next\",number)\n",
        "    \n",
        "    for i in range(2, number):\n",
        "        \n",
        "        if (int(number % i) == 0):\n",
        "           \n",
        "            i = number\n",
        "            print(\"check\",i)\n",
        "            break;\n",
        "\n",
        "    print(\"integervalue\",i)\n",
        "    print(\"numbervalue\",number)\n",
        "    if i is not number:\n",
        "      print(\"valuexxx\",number)\n",
        "      sum += number\n",
        "print(\"\\nThe sum of prime numbers in python from 1 to \", Last_number, \" is :\", sum)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 166
        },
        "id": "-PedhM0nPjA4",
        "outputId": "8e9f495f-39cc-406e-9f6c-4e1ed36314b7"
      },
      "outputs": [
        {
          "ename": "NameError",
          "evalue": "ignored",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-13-6611a20a8285>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0ma\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0mspark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msparkContext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparallelize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mArray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"a\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"a\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"b\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'Array' is not defined"
          ]
        }
      ],
      "source": [
        "a= spark.sparkContext.parallelize(Array((\"a\",1),(\"a\",2),(\"b\",2)))\n",
        "b =a.foldByKey(1)(_+_)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7J0ECeKTPjqq",
        "outputId": "7b3eed97-fdfb-44f2-ce53-beaa890f439d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[Row(r='1a2s3ae')]\n"
          ]
        }
      ],
      "source": [
        "import  pyspark.sql.functions as  F\n",
        "df=spark.createDataFrame([('translate',)], ['a']).select(F.translate('a', \"rnlt\", \"123\").alias('r')).collect()\n",
        "print(df)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y3B0EOW6c-rQ",
        "outputId": "dbc8bf8b-d09a-4627-cc46-da2e7b88b13c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+----+----+\n",
            "|Col1|Col2|\n",
            "+----+----+\n",
            "|   a|   1|\n",
            "|   b|   2|\n",
            "|   c|   3|\n",
            "+----+----+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "df = spark.createDataFrame([(\"a\", 1), (\"b\", 2), (\"c\",  3)], [\"Col1\", \"Col2\"])\n",
        "df.show()\n",
        "df.select(df.colRegex(\"`(Col1)?+.+`\")).show()"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "1LHZijXUq7sp3ubMGNQ9sx_NAnx5W4ziM",
      "authorship_tag": "ABX9TyNCrjZV23mZjKDw+2xG8JD1",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}