{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/poovendhiranr/Test_public_spark/blob/main/Test_spark.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EwTc4FoGwbPM",
        "outputId": "c43963d1-d7d0-4968-aa40-c1b3657450f7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting pyspark\n",
            "  Downloading pyspark-3.3.1.tar.gz (281.4 MB)\n",
            "\u001b[K     |████████████████████████████████| 281.4 MB 42 kB/s \n",
            "\u001b[?25hCollecting py4j==0.10.9.5\n",
            "  Downloading py4j-0.10.9.5-py2.py3-none-any.whl (199 kB)\n",
            "\u001b[K     |████████████████████████████████| 199 kB 48.1 MB/s \n",
            "\u001b[?25hBuilding wheels for collected packages: pyspark\n",
            "  Building wheel for pyspark (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyspark: filename=pyspark-3.3.1-py2.py3-none-any.whl size=281845512 sha256=74dfcd4ec832e39bd12fb0fc11d6cd655bcdd73eb8be77efc675bdf55730ccdd\n",
            "  Stored in directory: /root/.cache/pip/wheels/43/dc/11/ec201cd671da62fa9c5cc77078235e40722170ceba231d7598\n",
            "Successfully built pyspark\n",
            "Installing collected packages: py4j, pyspark\n",
            "Successfully installed py4j-0.10.9.5 pyspark-3.3.1\n"
          ]
        }
      ],
      "source": [
        "!pip3 install pyspark\n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.types import StructType, StructField, IntegerType, StringType, LongType\n",
        "import pyspark.sql.functions as f\n",
        "from pyspark.sql import Window\n",
        "spark = SparkSession.builder.master(\"local[*]\") \\\n",
        "                    .appName('Test_spark') \\\n",
        "                    .getOrCreate()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 356
        },
        "id": "48IaX-tVzbMz",
        "outputId": "2a5b040e-8441-40cf-b516-dbc2b240e55d"
      },
      "outputs": [
        {
          "ename": "ValueError",
          "evalue": "ignored",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-33-85a9c672ecd8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mrdd2\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrdd1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"|\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfilter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m!=\u001b[0m\u001b[0mheader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m[\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m6\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mrdd3\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcus_header\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrdd2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrdd3\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollectAsMap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/pyspark/rdd.py\u001b[0m in \u001b[0;36mcollectAsMap\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   2224\u001b[0m         \u001b[0;36m4\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2225\u001b[0m         \"\"\"\n\u001b[0;32m-> 2226\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2227\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2228\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mkeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m\"RDD[Tuple[K, V]]\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;34m\"RDD[K]\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: dictionary update sequence element #0 has length 3; 2 is required"
          ]
        }
      ],
      "source": [
        "rdd1=spark.sparkContext.textFile('/content/property.csv')\n",
        "header=rdd1.map(lambda x:x.split(\"|\")).first()\n",
        "cus_header=spark.sparkContext.parallelize([['popid','size','final']])\n",
        "rdd2=rdd1.map(lambda x:x.split(\"|\")).filter(lambda x:x!=header).map(lambda x:[ x[0],x[5],str(int(x[5]) * int(x[6])) ])\n",
        "rdd3=cus_header.union(rdd2)\n",
        "for i in rdd3.collectAsMap():\n",
        "  print(i)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "kVJbmHak_Kit",
        "outputId": "b490bd10-2f4f-4007-dad6-859546bc3edc"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "100\n"
          ]
        }
      ],
      "source": [
        "df1=spark.range(1,500000).toDF(\"id1\").withColumn(\"C1\",f.lit(\"hello\"))\n",
        "df2=spark.range(200,600000).toDF(\"id2\").withColumn(\"C2\",f.lit(\"hi\"))\n",
        "df3=df1.join(df2,df1.id1==df2.id2,how=\"inner\")\n",
        "print(df3.rdd.getNumPartitions())\n",
        "df3.coals(300).write.mode(\"overwrite\").save(\"/content/check.csv\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cFzagTcmpU2R",
        "outputId": "9f0538bf-e406-421c-ea11-d0252c2f2093"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[0, 0, 0, 0, 1, 1, 1, 1]\n"
          ]
        }
      ],
      "source": [
        "# df1 = spark.read.format(\"csv\") \\\n",
        "#             .option(\"delimiter\", \"|\") \\\n",
        "#             .option(\"header\", \"true\") \\\n",
        "#             .option(\"inferSchema\", \"True\") \\\n",
        "#             .load(\"/content/oth.csv\").createOrReplaceTempView(\"hello\")\n",
        "\n",
        "# df2=df1.drop(\"_c0\")\n",
        "# df3=df2.groupBy(\"id\",\"name\").agg(f.min(df2.age).alias (\"age\") ,f.min(df2.city))\n",
        "# df3.show()\n",
        "\n",
        "# spark.sql('''select id, name,min(age) as age ,min(city) as city from hello group by id,name''').show()\n",
        "# df1 = spark.read.format(\"csv\") \\\n",
        "#             .option(\"delimiter\", \"|\") \\\n",
        "#             .option(\"header\", \"true\") \\\n",
        "#             .option(\"inferSchema\", \"True\") \\\n",
        "#             .load(\"/content/oth1.csv\")\n",
        "         \n",
        "#spark.sql('''select Name,Weight from (select Name,Weight,dense_rank() over(order by Weight desc) as rnk from hello )temp where rnk<3 ''').show()\n",
        "\n",
        "\n",
        "\n",
        "# wd=Window.orderBy(f.col(\"Weight\").desc())\n",
        "# df1.withColumn(\"rnk\",f.rank().over(wd)).filter(f.col(\"rnk\")<3).select(f.col(\"Name\"),f.col(\"Weight\")).show()\n",
        "\n",
        "# rdd1=spark.sparkContext.textFile(\"/content/oth1.csv\")\n",
        "# header = rdd1.map(lambda x:x.split('|')).first()\n",
        "# ab=rdd1.map(lambda x:x.split('|')).filter(lambda x: x!=header) \\\n",
        "#        .sortBy(lambda X:X[1],False) \\\n",
        "#        .filter(lambda x : int(x[1])>60)\n",
        "# for a in ab.collect():\n",
        "#   print(a)\n",
        "\n",
        "\n",
        "# ab=rdd1.map(lambda x:x.split('|')).filter(lambda x: x!=header) \\\n",
        "#        .sortBy(lambda X:X[1],False) \\\n",
        "#        .take(2)\n",
        "# print(ab[1])\n",
        "\n",
        "\n",
        "# df1 = spark.read.format(\"csv\") \\\n",
        "#             .option(\"delimiter\", \",\") \\\n",
        "#             .option(\"header\", \"true\") \\\n",
        "#             .option(\"inferSchema\", \"True\") \\\n",
        "#             .load(\"/content/oth2.csv\")\n",
        "# df1.withColumn(\"hob\",f.explode(f.split(f.col(\"hobbies\"),'-'))) \\\n",
        "#    .drop(f.col(\"hobbies\")).show()\n",
        "\n",
        "# rdd1=spark.sparkContext.textFile(\"/content/oth2.csv\")\n",
        "# header = rdd1.map(lambda x:x.split(',')).first()\n",
        "# ab=rdd1.map(lambda x:x.split(',')).filter(lambda x: x!=header).map(lambda x :[x[0],x[1],x[2].split(\"-\")]) \n",
        "# lis=[]\n",
        "# for a in ab.collect():\n",
        "#   for b in a[2]:\n",
        "#     lis=lis+[a[0],a[1],b]\n",
        "# print(lis)    \n",
        "\n",
        "\n",
        "\n",
        "# data = [(\"James\",\"Sales\",34), (\"Michael\",\"Sales\",56),(\"Robert\",\"Sales\",30), (\"Maria\",\"Finance\",24) ]\n",
        "# df1 = data.toDF(\"name\",\"dept\",\"age\")\n",
        "# df1.printSchema()\n",
        "\n",
        "# data2=Seq((\"James\",\"Sales\",\"NY\",9000),(\"Maria\",\"Finance\",\"CA\",9000),(\"Jen\",\"Finance\",\"NY\",7900),(\"Jeff\",\"Marketing\",\"CA\",8000))\n",
        "# df2 = data2.toDF(\"name\",\"dept\",\"state\",\"salary\")\n",
        "# df2.printSchema()\n",
        "\n",
        "\n",
        "# lis=[1,0,1,0,1,0,1,0]\n",
        "# ad=[]\n",
        "# for i in lis: \n",
        "#   if i<1:\n",
        "#     ad=ad+[i]\n",
        "# for i in lis:\n",
        "#   if i>0:\n",
        "#     ad=ad+[i]\n",
        "# print(ad)       \n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "SESSION_Allocation"
      ],
      "metadata": {
        "id": "6Zfp-MT2izp9"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KpY9O6lqxxHY",
        "outputId": "c31be911-6958-4bc3-9b22-207fee97be52"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---+-------------------+-------------------+------------------+------------------+----------------+-------------------+---------------+-----+\n",
            "|idt|dtime              |prev_dtime         |session_diff_30min|session_flag_30min|session_id_30min|min_dtime          |session_id_2hrs|total|\n",
            "+---+-------------------+-------------------+------------------+------------------+----------------+-------------------+---------------+-----+\n",
            "|U1 |2020-03-28 03:45:00|null               |null              |1                 |1               |2020-03-28 03:45:00|0              |1    |\n",
            "|U1 |2020-03-28 04:10:00|2020-03-28 03:45:00|25.0              |0                 |1               |2020-03-28 03:45:00|0              |1    |\n",
            "|U1 |2020-03-28 04:35:00|2020-03-28 04:10:00|25.0              |0                 |1               |2020-03-28 03:45:00|0              |1    |\n",
            "|U1 |2020-03-28 04:55:00|2020-03-28 04:35:00|20.0              |0                 |1               |2020-03-28 03:45:00|0              |1    |\n",
            "|U1 |2020-03-28 05:20:00|2020-03-28 04:55:00|25.0              |0                 |1               |2020-03-28 03:45:00|0              |1    |\n",
            "|U1 |2020-03-28 05:45:00|2020-03-28 05:20:00|25.0              |0                 |1               |2020-03-28 03:45:00|1              |2    |\n",
            "|U1 |2020-03-28 06:10:00|2020-03-28 05:45:00|25.0              |0                 |1               |2020-03-28 03:45:00|1              |2    |\n",
            "|U1 |2020-03-28 06:58:00|2020-03-28 06:10:00|48.0              |1                 |2               |2020-03-28 03:45:00|1              |3    |\n",
            "|U1 |2020-03-28 07:15:00|2020-03-28 06:58:00|17.0              |0                 |2               |2020-03-28 03:45:00|1              |3    |\n",
            "|U1 |2020-03-28 07:25:00|2020-03-28 07:15:00|10.0              |0                 |2               |2020-03-28 03:45:00|1              |3    |\n",
            "|U1 |2020-03-28 07:30:00|2020-03-28 07:25:00|5.0               |0                 |2               |2020-03-28 03:45:00|1              |3    |\n",
            "|U1 |2020-03-28 09:35:00|2020-03-28 07:30:00|125.0             |1                 |3               |2020-03-28 03:45:00|1              |4    |\n",
            "|U1 |2020-03-28 10:00:00|2020-03-28 09:35:00|25.0              |0                 |3               |2020-03-28 03:45:00|1              |4    |\n",
            "|U1 |2020-03-28 10:10:00|2020-03-28 10:00:00|10.0              |0                 |3               |2020-03-28 03:45:00|1              |4    |\n",
            "|U1 |2020-03-28 12:15:00|2020-03-28 10:10:00|125.0             |1                 |4               |2020-03-28 03:45:00|1              |5    |\n",
            "|U2 |2020-03-28 09:35:00|null               |null              |1                 |1               |2020-03-28 09:35:00|0              |1    |\n",
            "|U2 |2020-03-28 10:00:00|2020-03-28 09:35:00|25.0              |0                 |1               |2020-03-28 09:35:00|0              |1    |\n",
            "|U2 |2020-03-28 10:40:00|2020-03-28 10:00:00|40.0              |1                 |2               |2020-03-28 09:35:00|0              |2    |\n",
            "+---+-------------------+-------------------+------------------+------------------+----------------+-------------------+---------------+-----+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "\n",
        "df1 = spark.read.format(\"csv\") \\\n",
        "            .option(\"delimiter\", \",\") \\\n",
        "            .option(\"header\", \"true\") \\\n",
        "            .option(\"TimeStampFormat\", \"yyyy-MM-dd HH:mm\") \\\n",
        "            .option(\"inferSchema\", \"True\") \\\n",
        "            .load(\"/content/session2.csv\")\n",
        "\n",
        "# populate prev dtime for each record\n",
        "df_30 = df1.withColumn(\"prev_dtime\",f.lag(\"dtime\").over(Window.partitionBy(\"idt\").orderBy(\"dtime\")),)\n",
        "\n",
        "#find the diff of dtime and prev_dtime if it is less than or equal to 30 min add 0 else add 1\n",
        "df_30 = df_30.withColumn(\"session_diff_30min\", \\\n",
        "                   (f.col(\"dtime\").cast(LongType()) - f.col(\"prev_dtime\").cast(LongType()))/60)\n",
        "df_30 = df_30.withColumn(\"session_flag_30min\", \\\n",
        "                  f.when(f.col(\"session_diff_30min\")<30,0).otherwise(1))\n",
        "\n",
        "df_30 = df_30.withColumn(\"session_id_30min\",f.sum(\"session_flag_30min\").over(Window.partitionBy(\"idt\").orderBy(\"dtime\")))\n",
        "\n",
        "#df_30_mindt = df_30.groupBy(\"idt\", \"session_id_30min\").agg(f.min(f.col(\"dtime\")).alias(\"min_dtime\"))\n",
        "df_30_mindt = df_30.groupBy(\"idt\").agg(f.min(f.col(\"dtime\")).alias(\"min_dtime\"))\n",
        "\n",
        "df_2hrs_1 = df_30.join(df_30_mindt, [\"idt\" ], \"inner\")\n",
        "df_2hrs_1=df_2hrs_1.withColumn(\"session_id_2hrs\", \\\n",
        "                   f.when(f.floor((f.col(\"dtime\").cast(LongType()) - f.col(\"min_dtime\").cast(LongType()))/7200)>=1,1).otherwise(0))\n",
        "\n",
        "df_2hrs_1=df_2hrs_1.withColumn(\"total\",f.col(\"session_id_2hrs\")+f.col(\"session_id_30min\"))\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# create the session id using the running total\n",
        "# df_30 = df_30.withColumn(\"session_id_30min\",f.sum(\"session_flag_30min\").over(Window.partitionBy(\"idt\").orderBy(\"dtime\")))\n",
        "# df_30.show(truncate=False)\n",
        "\n",
        "#now we have completed the creating the session id with 30 min logic\n",
        "#start implement the 2hrs logic\n",
        "#for each user and session id get the minimum date time\n",
        "# df_30_mindt = df_30.groupBy(\"idt\", \"session_id_30min\").agg(f.min(f.col(\"dtime\")).alias(\"min_dtime\"))\n",
        "# df_30_mindt.show(truncate=False)\n",
        "\n",
        "#now join df_30 with df_30mindt to populate the min_dtime column\n",
        "# df_2hrs_1 = df_30.join(df_30_mindt, [\"idt\" , \"session_id_30min\"], \"inner\")\n",
        "\n",
        "# #now find the differeent between dtime and min_dtime if the difference is less than 7200 (2HRS) add 0 else 1\n",
        "# df_2hrs_2 = df_2hrs_1.withColumn(\"session_flag_2hrs\", \\\n",
        "#         f.when(f.col(\"dtime\").cast(LongType()) - f.col(\"min_dtime\").cast(LongType()) <= 7200, 0).otherwise(1))\n",
        "\n",
        "\n",
        "# df_2hrs_2 = df_2hrs_2.withColumn(\"session_flag_2hrs\",f.sum(\"session_flag_2hrs\").over(Window.partitionBy(\"idt\").orderBy(\"dtime\")))\n",
        "\n",
        "\n",
        "# #now to populate the final session id sum session_flag_30min and session_flag_2hrs\n",
        "#df_final = df_2hrs_1.withColumn(\"total\", f.col(\"session_id_30min\") + f.col(\"session_id_2hrs\"))\n",
        "\n",
        "df_2hrs_1.show(truncate=False)\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 190
        },
        "id": "ijnRcDpfnyvs",
        "outputId": "67261a4b-4599-41e4-cff6-94d575eb8439"
      },
      "outputs": [
        {
          "ename": "AttributeError",
          "evalue": "ignored",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-9-9a938be656f1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdf_30\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf_30\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwithColumn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"session_id_30min\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"session_flag_30min\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mover\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mWindow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpartitionBy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"idt\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0morderBy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"dtime\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mdf_30\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtruncate\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'withColumn'"
          ]
        }
      ],
      "source": [
        "df1 = spark.read.format(\"csv\") \\\n",
        "            .option(\"delimiter\", \",\") \\\n",
        "            .option(\"header\", \"true\") \\\n",
        "            .option(\"TimeStampFormat\", \"yyyy-MM-dd HH:mm\") \\\n",
        "            .option(\"inferSchema\", \"True\") \\\n",
        "            .load(\"/content/session2.csv\")\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8z92yUS12GbK"
      },
      "outputs": [],
      "source": [
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BUIyskNRueFY",
        "outputId": "0b2ea325-a17e-40e4-e271-c17187a914a6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+-----------------+--------+\n",
            "|        avg(Diff)| trip_id|\n",
            "+-----------------+--------+\n",
            "|7.888888888888889|trip_006|\n",
            "+-----------------+--------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "df=spark.read.format(\"csv\").option(\"delimiter\", \",\").option(\"header\",True).load(\"/content/fuel.csv\")\n",
        "df1=df.withColumn('total_min',F.hour(F.col(\"timestamp\"))*60+F.minute(F.col(\"timestamp\"))).createOrReplaceTempView(\"temp1\")\n",
        "df2=spark.sql('''\n",
        "select trip_id,timestamp,fuel_level,LEAD(fuel_level ,1,0) \n",
        "OVER ( ORDER BY timestamp,0) AS compare_fuel,\n",
        "(fuel_level-LEAD(fuel_level ,1,0) \n",
        "OVER ( ORDER BY timestamp,0)) as Diff\n",
        "from temp1''').createOrReplaceTempView(\"temp2\")\n",
        "df3=spark.sql('''select avg(Diff),trip_id  from temp2 group by trip_id ''').show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sjpaR3xuXGOG",
        "outputId": "36327595-9f66-41cf-902d-28b2cc4ba14d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+---+-----------+---------+\n",
            "|idf|sum(amount)|occurance|\n",
            "+---+-----------+---------+\n",
            "|101|      600.0|        2|\n",
            "+---+-----------+---------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "df_1=spark.read.format(\"csv\").option(\"header\",True).load(\"/content/check_22.csv\")\n",
        "df=df_1.createOrReplaceTempView(\"check7\")\n",
        "fg=spark.sql('''select idf,sum(amount),count(idf) as occurance from check7 where idf in \n",
        "(select i.idf from check7 as i group by idf having count( distinct orderdate) >1)\n",
        " group by idf ''').createOrReplaceTempView(\"check77\")\n",
        "gg=spark.sql('''select * from check77''').show()\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yYkzlprDitGa",
        "outputId": "15360bdb-2c52-4000-b3e7-5cb8b609d2ed"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[Row(r='1a2s3ae')]\n"
          ]
        }
      ],
      "source": [
        "df=spark.createDataFrame([('translate',)], ['a']).select(F.translate('a', \"rnlt\", \"123\").alias('r')).collect()\n",
        "print(df)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FF_GSlViisvD"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 201
        },
        "id": "iataIps8iHFi",
        "outputId": "504773bf-f4ee-41ea-ab03-93f181535cb8"
      },
      "outputs": [
        {
          "ename": "NameError",
          "evalue": "ignored",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-23-f75a89c0ba8f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mdF\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mspark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreateDataFrame\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mRow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mintlist\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmapfield\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m\"a\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m\"b\"\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mselect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexplode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mintlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0malias\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"anInt\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'Row' is not defined"
          ]
        }
      ],
      "source": [
        "dF = spark.createDataFrame\n",
        "([Row(a=1, intlist=[1,2,3], mapfield={\"a\": \"b\"})])\n",
        "df.select(F.explode(df.intlist).alias(\"anInt\")).collect()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PEYd9VnKiG5J"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 363
        },
        "id": "MQDQ9Pwb0p51",
        "outputId": "8d4d0a7b-2de8-4eae-ea0f-e71f93575215"
      },
      "outputs": [
        {
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-4-cc3a00ad19cf>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mpyspark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msql\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mSparkSession\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mspark\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSparkSession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuilder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmaster\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"local[*]\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m                     \u001b[0;34m.\u001b[0m\u001b[0mappName\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Test_spark'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m                     \u001b[0;34m.\u001b[0m\u001b[0mgetOrCreate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/pyspark/sql/session.py\u001b[0m in \u001b[0;36mgetOrCreate\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    267\u001b[0m                         \u001b[0msparkConf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    268\u001b[0m                     \u001b[0;31m# This SparkContext may be an existing one.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 269\u001b[0;31m                     \u001b[0msc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSparkContext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetOrCreate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msparkConf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    270\u001b[0m                     \u001b[0;31m# Do not update `SparkConf` for existing `SparkContext`, as it's shared\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    271\u001b[0m                     \u001b[0;31m# by all sessions.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/pyspark/context.py\u001b[0m in \u001b[0;36mgetOrCreate\u001b[0;34m(cls, conf)\u001b[0m\n\u001b[1;32m    481\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mSparkContext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    482\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mSparkContext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_active_spark_context\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 483\u001b[0;31m                 \u001b[0mSparkContext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconf\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconf\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mSparkConf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    484\u001b[0m             \u001b[0;32massert\u001b[0m \u001b[0mSparkContext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_active_spark_context\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    485\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mSparkContext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_active_spark_context\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/pyspark/context.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, master, appName, sparkHome, pyFiles, environment, batchSize, serializer, conf, gateway, jsc, profiler_cls, udf_profiler_cls)\u001b[0m\n\u001b[1;32m    193\u001b[0m             )\n\u001b[1;32m    194\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 195\u001b[0;31m         \u001b[0mSparkContext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_ensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgateway\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mgateway\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconf\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    196\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    197\u001b[0m             self._do_init(\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/pyspark/context.py\u001b[0m in \u001b[0;36m_ensure_initialized\u001b[0;34m(cls, instance, gateway, conf)\u001b[0m\n\u001b[1;32m    415\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mSparkContext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    416\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mSparkContext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_gateway\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 417\u001b[0;31m                 \u001b[0mSparkContext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_gateway\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgateway\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mlaunch_gateway\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    418\u001b[0m                 \u001b[0mSparkContext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jvm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSparkContext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_gateway\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjvm\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    419\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/pyspark/java_gateway.py\u001b[0m in \u001b[0;36mlaunch_gateway\u001b[0;34m(conf, popen_kwargs)\u001b[0m\n\u001b[1;32m    101\u001b[0m             \u001b[0;31m# Wait for the file to appear, or for the process to exit, whichever happens first.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    102\u001b[0m             \u001b[0;32mwhile\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mproc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpoll\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misfile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconn_info_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 103\u001b[0;31m                 \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msleep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0.1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    104\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    105\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misfile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconn_info_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "from pyspark.sql import SparkSession\n",
        "spark = SparkSession.builder.master(\"local[*]\") \\\n",
        "                    .appName('Test_spark') \\\n",
        "                    .getOrCreate()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 105
        },
        "id": "3spwYnP29klD",
        "outputId": "7b409adc-8eb4-4579-f7d8-2618b0315d86"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "H\n",
            "ello\n",
            "W\n",
            "orld\n"
          ]
        },
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'Hello World '"
            ]
          },
          "execution_count": 13,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "def convertCase(str):\n",
        "    resStr=\"\"\n",
        "    arr = str.split(\" \")\n",
        "    for x in arr:\n",
        "       print (x[0:1].upper())\n",
        "       print( x[1:len(x)])\n",
        "       resStr= resStr + x[0:1].upper() + x[1:len(x)] + \" \"\n",
        "       \n",
        "    return resStr\n",
        "\n",
        "convertCase(\"hello world\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7XAtePFv3B2Y"
      },
      "outputs": [],
      "source": [
        "import pyspark\n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.functions import col, udf\n",
        "from pyspark.sql.types import StringType\n",
        "\n",
        "spark = SparkSession.builder.appName('SparkByExamples.com').getOrCreate()\n",
        "\n",
        "columns = [\"Seqno\",\"Name\"]\n",
        "data = [(\"1\", \"john jones\"),\n",
        "    (\"2\", \"tracey smith\"),\n",
        "    (\"3\", \"amy sanders\")]\n",
        "\n",
        "df = spark.createDataFrame(data=data,schema=columns)\n",
        "\n",
        "df.show(truncate=False)\n",
        "\n",
        "def convertCase(str):\n",
        "    resStr=\"\"\n",
        "    arr = str.split(\" \")\n",
        "    for x in arr:\n",
        "       resStr= resStr + x[0:1].upper() + x[1:len(x)] + \" \"\n",
        "    return resStr \n",
        "\n",
        "\"\"\" Converting function to UDF \"\"\"\n",
        "convertUDF = udf(lambda z: convertCase(z))\n",
        "\n",
        "df.select(col(\"Seqno\"), \\\n",
        "    convertUDF(col(\"Name\")).alias(\"Name\") ) \\\n",
        ".show(truncate=False)\n",
        "\n",
        "def upperCase(str):\n",
        "    return str.upper()\n",
        "\n",
        "upperCaseUDF = udf(lambda z:upperCase(z),StringType())    \n",
        "\n",
        "df.withColumn(\"Cureated Name\", upperCaseUDF(col(\"Name\"))) \\\n",
        ".show(truncate=False)\n",
        "\n",
        "\"\"\" Using UDF on SQL \"\"\"\n",
        "spark.udf.register(\"convertUDF\", convertCase,StringType())\n",
        "df.createOrReplaceTempView(\"NAME_TABLE\")\n",
        "spark.sql(\"select Seqno, convertUDF(Name) as Name from NAME_TABLE\") \\\n",
        "     .show(truncate=False)\n",
        "     \n",
        "spark.sql(\"select Seqno, convertUDF(Name) as Name from NAME_TABLE \" + \\\n",
        "          \"where Name is not null and convertUDF(Name) like '%John%'\") \\\n",
        "     .show(truncate=False)  \n",
        "     \n",
        "\"\"\" null check \"\"\"\n",
        "\n",
        "columns = [\"Seqno\",\"Name\"]\n",
        "data = [(\"1\", \"john jones\"),\n",
        "    (\"2\", \"tracey smith\"),\n",
        "    (\"3\", \"amy sanders\"),\n",
        "    ('4',None)]\n",
        "\n",
        "df2 = spark.createDataFrame(data=data,schema=columns)\n",
        "df2.show(truncate=False)\n",
        "df2.createOrReplaceTempView(\"NAME_TABLE2\")\n",
        "    \n",
        "spark.udf.register(\"_nullsafeUDF\", lambda str: convertCase(str) if not str is None else \"\" , StringType())\n",
        "\n",
        "spark.sql(\"select _nullsafeUDF(Name) from NAME_TABLE2\") \\\n",
        "     .show(truncate=False)\n",
        "\n",
        "spark.sql(\"select Seqno, _nullsafeUDF(Name) as Name from NAME_TABLE2 \" + \\\n",
        "          \" where Name is not null and _nullsafeUDF(Name) like '%John%'\") \\\n",
        "     .show(truncate=False)  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IL93kXddYGbi",
        "outputId": "c28881f7-61f6-47e4-bf75-a79d2f22979d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+-----+------------+\n",
            "|Seqno|Name        |\n",
            "+-----+------------+\n",
            "|1    |john jones  |\n",
            "|2    |tracey smith|\n",
            "|3    |amy sanders |\n",
            "|4    |null        |\n",
            "+-----+------------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "\n",
        "import pyspark\n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.functions import col, udf\n",
        "from pyspark.sql import functions as F\n",
        "from pyspark.sql.types import StringType\n",
        "\n",
        "spark = SparkSession.builder.appName('SparkByExamples.com').getOrCreate()\n",
        "columns = [\"Seqno\",\"Name\"]\n",
        "data = [(\"1\", \"john jones\"),\n",
        "    (\"2\", \"tracey smith\"),\n",
        "    (\"3\", \"amy sanders\"),\n",
        "    ('4',None)]\n",
        "\n",
        "df2 = spark.createDataFrame(data=data,schema=columns)\n",
        "df2.show(truncate=False)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 311
        },
        "id": "fsCwHGnRYKzY",
        "outputId": "df3f9624-5047-486b-80b7-0ebee476c135"
      },
      "outputs": [
        {
          "ename": "TypeError",
          "evalue": "ignored",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-24-07d8a00369f9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdf2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mselect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mregexp_replace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcol\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m' '\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m''\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0malias\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mregexp_replace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcname\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m' '\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m''\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mcname\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdf2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-24-07d8a00369f9>\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdf2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mselect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mregexp_replace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcol\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m' '\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m''\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0malias\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mregexp_replace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcname\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m' '\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m''\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mcname\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdf2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pyspark/sql/column.py\u001b[0m in \u001b[0;36malias\u001b[0;34m(self, *alias, **kwargs)\u001b[0m\n\u001b[1;32m    881\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mColumn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"as\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0malias\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mjmeta\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    882\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 883\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mColumn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"as\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0malias\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    884\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    885\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mmetadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1311\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1312\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1313\u001b[0;31m         \u001b[0margs_command\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtemp_args\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_build_args\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1314\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1315\u001b[0m         \u001b[0mcommand\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mproto\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCALL_COMMAND_NAME\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/py4j/java_gateway.py\u001b[0m in \u001b[0;36m_build_args\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1275\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_build_args\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1276\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconverters\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconverters\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1277\u001b[0;31m             \u001b[0;34m(\u001b[0m\u001b[0mnew_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_args\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1278\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1279\u001b[0m             \u001b[0mnew_args\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/py4j/java_gateway.py\u001b[0m in \u001b[0;36m_get_args\u001b[0;34m(self, args)\u001b[0m\n\u001b[1;32m   1262\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0mconverter\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconverters\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1263\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0mconverter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcan_convert\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1264\u001b[0;31m                         \u001b[0mtemp_arg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconverter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconvert\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1265\u001b[0m                         \u001b[0mtemp_args\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtemp_arg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1266\u001b[0m                         \u001b[0mnew_args\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtemp_arg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/py4j/java_collections.py\u001b[0m in \u001b[0;36mconvert\u001b[0;34m(self, object, gateway_client)\u001b[0m\n\u001b[1;32m    508\u001b[0m         \u001b[0mArrayList\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mJavaClass\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"java.util.ArrayList\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgateway_client\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    509\u001b[0m         \u001b[0mjava_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mArrayList\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 510\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0melement\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mobject\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    511\u001b[0m             \u001b[0mjava_list\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0melement\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    512\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mjava_list\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pyspark/sql/column.py\u001b[0m in \u001b[0;36m__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    558\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    559\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__iter__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 560\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Column is not iterable\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    561\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    562\u001b[0m     \u001b[0;31m# string methods\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: Column is not iterable"
          ]
        }
      ],
      "source": [
        "df2.select([F.regexp_replace(F.col(cname),' ','').alias (F.regexp_replace(cname,' ','')) for cname in df2.columns]).show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LiMq_FoGZ2bS"
      },
      "outputs": [],
      "source": [
        "# prime number testing\n",
        "Last_number=12\n",
        "sum=0\n",
        "\n",
        "for number in range(2, Last_number + 1):\n",
        "    print(\"next\",number)\n",
        "    \n",
        "    for i in range(2, number):\n",
        "        \n",
        "        if (int(number % i) == 0):\n",
        "           \n",
        "            i = number\n",
        "            print(\"check\",i)\n",
        "            break;\n",
        "\n",
        "    print(\"integervalue\",i)\n",
        "    print(\"numbervalue\",number)\n",
        "    if i is not number:\n",
        "      print(\"valuexxx\",number)\n",
        "      sum += number\n",
        "print(\"\\nThe sum of prime numbers in python from 1 to \", Last_number, \" is :\", sum)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 166
        },
        "id": "-PedhM0nPjA4",
        "outputId": "8e9f495f-39cc-406e-9f6c-4e1ed36314b7"
      },
      "outputs": [
        {
          "ename": "NameError",
          "evalue": "ignored",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-13-6611a20a8285>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0ma\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0mspark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msparkContext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparallelize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mArray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"a\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"a\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"b\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'Array' is not defined"
          ]
        }
      ],
      "source": [
        "a= spark.sparkContext.parallelize(Array((\"a\",1),(\"a\",2),(\"b\",2)))\n",
        "b =a.foldByKey(1)(_+_)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7J0ECeKTPjqq",
        "outputId": "7b3eed97-fdfb-44f2-ce53-beaa890f439d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[Row(r='1a2s3ae')]\n"
          ]
        }
      ],
      "source": [
        "import  pyspark.sql.functions as  F\n",
        "df=spark.createDataFrame([('translate',)], ['a']).select(F.translate('a', \"rnlt\", \"123\").alias('r')).collect()\n",
        "print(df)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y3B0EOW6c-rQ",
        "outputId": "dbc8bf8b-d09a-4627-cc46-da2e7b88b13c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+----+----+\n",
            "|Col1|Col2|\n",
            "+----+----+\n",
            "|   a|   1|\n",
            "|   b|   2|\n",
            "|   c|   3|\n",
            "+----+----+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "df = spark.createDataFrame([(\"a\", 1), (\"b\", 2), (\"c\",  3)], [\"Col1\", \"Col2\"])\n",
        "df.show()\n",
        "df.select(df.colRegex(\"`(Col1)?+.+`\")).show()"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNFPn59s9HndJMAkLy5tBcN",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}